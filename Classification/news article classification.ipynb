{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "labels = ['politics','sport','health','money','local']\n",
    "\n",
    "if os.path.exists(\"../data/data_df.csv\"):\n",
    "    data_df = pd.DataFrame.from_csv(\"../data/data_df.csv\")\n",
    "else:\n",
    "    data = []\n",
    "    for l in labels:\n",
    "        for name in os.listdir(data_dir + l):\n",
    "            article = json.load(open(data_dir + l + \"/\" + name))\n",
    "            text = article['text']\n",
    "            if text.endswith(\"Read More\"):\n",
    "                text = text[:-len(\"Read More\")]\n",
    "            art_list = [article['title'],text,l]\n",
    "            data.append(art_list)\n",
    "    data = np.array(data)\n",
    "    data_df = pd.DataFrame(data,columns=['title','text','label'])\n",
    "    data_df.to_csv(\"../data/data_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "1.  categories into discrete numerical values;\n",
    "2.  Transform all words to lowercase;\n",
    "3.  Remove all punctuations and stopwords.\n",
    "4.  Tokenization and stemming the tokens.\n",
    "5.  Replaced numerical values with '#num#' to reduce vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.porter import *\n",
    "import nltk \n",
    "import re\n",
    "\n",
    "if (!os.path.exists(\"../data/data_df_processed.csv\"):\n",
    "    data_df_processed = data_df.copy()\n",
    "    data_df_processed['label'] = data_df.label.map({ 'local':0, 'politics': 1, 'sport': 2, 'health': 3, 'money': 4})\n",
    "    data_df_processed['title'] = data_df.title.map(\n",
    "        lambda x: x.lower().translate(str.maketrans('','', string.punctuation))\n",
    "    )\n",
    "    data_df_processed['text'] = data_df.text.map(\n",
    "        lambda x: x.lower().translate(str.maketrans('','', string.punctuation))\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    data_df_processed['title_stem'] = data_df_processed.title.map(\n",
    "        lambda x: ' '.join([stemmer.stem(w) for w in nltk.word_tokenize(x) if w not in stopwords.words('english')]))\n",
    "\n",
    "    data_df_processed['text_stem'] = data_df_processed.text.map(\n",
    "        lambda x: ' '.join([stemmer.stem(w) for w in nltk.word_tokenize(x) if w not in stopwords.words('english')]))\n",
    "\n",
    "    regex = re.compile(r\"\\d+\", re.IGNORECASE)\n",
    "\n",
    "    data_df_processed['title_stem'] = data_df_processed.title_stem.map(\n",
    "        lambda x: regex.sub(\"spnumsp\", x))\n",
    "\n",
    "    data_df_processed['text_stem'] = data_df_processed.text_stem.map(\n",
    "        lambda x: regex.sub(\"spnumsp\", x))\n",
    "    \n",
    "    data_df_processed['title+text_stem'] = data_df_processed['text_stem'].values + data_df_processed['title_stem'].values\n",
    "    \n",
    "    data_df_processed['title+text'] = data_df_processed['text'].values + data_df_processed['title'].values\n",
    "    \n",
    "    data_df_processed.to_csv(\"../data/data_df_processed.csv\")\n",
    "else:\n",
    "    data_df_processed = pd.DataFrame.from_csv(\"../data/data_df_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_processed['title_nonum'] = data_df_processed.title.map(\n",
    "    lambda x: regex.sub(\"spnumsp\", x))\n",
    "\n",
    "data_df_processed['text_nonum'] = data_df_processed.text.map(\n",
    "    lambda x: regex.sub(\"spnumsp\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  (7608,)\n",
      "Test dataset:  (2537,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#     data_df_processed['text_nonum'].values + data_df_processed['title_nonum'].values , \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_df_processed['title+text'].values,\n",
    "    data_df_processed['label'].values, \n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "print(\"Training dataset: \", X_train.shape)\n",
    "print(\"Test dataset: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "Apply bag of words processing to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vector = CountVectorizer(stop_words = 'english', ngram_range=(1, 4),  max_features=None, min_df=2)\n",
    "# count_vector = CountVectorizer(stop_words = 'english')\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.90185258179\n",
      "Recall score:  0.90185258179\n",
      "Precision score:  0.902260376079\n",
      "F1 score:  0.901933255216\n"
     ]
    }
   ],
   "source": [
    "predictions = naive_bayes.predict(testing_data)\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print(\"Recall score: \", recall_score(y_test, predictions, average = 'weighted'))\n",
    "print(\"Precision score: \", precision_score(y_test, predictions, average = 'weighted'))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no stem, text+title, 0.880961765865\n",
    "# no stem no number , text+title, 0.881355932203\n",
    "# stem, text+title, 0.871501773749\n",
    "\n",
    "# no stem no number , text+title, ngram_range=(1, 2),  max_features=None, min_df = 5, 0.885691761924\n",
    "# no stem, text+title, ngram_range=(1, 2),  max_features=None, min_df = 5, 0.889239258967\n",
    "# no stem, text+title, ngram_range=(1, 2),  max_features=None, min_df = 10, 0.878990934174\n",
    "# no stem, text+title, ngram_range=(1, 2),  max_features=None, min_df = 8, 0.884509262909\n",
    "# no stem, text+title, ngram_range=(1, 2),  max_features=None, min_df = 3, 0.896728419393\n",
    "# no stem, text+title, ngram_range=(1, 2),  max_features=None, min_df = 2, 0.899881750099\n",
    "# no stem, text+title, ngram_range=(1, 2),  max_features=None, min_df = 1, 0.882538431218\n",
    "# no stem, text+title, ngram_range=(1, 2),  max_features=None, min_df = 4, 0.891604256996\n",
    "# no stem, text+title, ngram_range=(1, 3),  max_features=None, min_df = 2, 0.900670082775\n",
    "# no stem, text+title, ngram_range=(1, 4),  max_features=None, min_df = 2, 0.90185258179\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
